# Metadata

## model_name
<!--- Name of the model -->
Yi 34B Chat

## model_num_parameters
<!--- Number of Parameters -->
34B

## model_creator
<!--- Creator of the model -->
01.AI

## model_version
<!--- Used model version -->
01-ai/Yi-34B-Chat

## model_published
<!--- When was the model published -->
November 2023

## model_type
<!--- {api, api_with_logit, local} -->
local

## model_link
<!--- Link to the model -->
https://huggingface.co/01-ai/Yi-34B-Chat

## model_description
<!--- Short description of the model -->
The Yi series of models are a collection of pretrained and fine-tuned generative text models trained on a 3T token bilungual corpus, inclduing both English and Chinese texts. It is based on the Llama architecture and achieved various state-of-the-art results for open models on a wide range of benchmarks.

# environment

## num_gpus
<!--- number of gpu's used --> 
-

## gpu_power_draw
<!--- draw of the used GPUs in kW --> 
-

## time_to_train
<!--- total time taken for training in hours --> 
-

## datacenter_carbon_intensity
<!--- grams of CO2 emissions per kWh of energy consumed of the datacenter -->
-