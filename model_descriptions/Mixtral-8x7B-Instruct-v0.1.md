# Metadata

## model_name
<!--- Name of the model -->
Mistral 8x7B Instruct

## model_num_parameters
<!--- Number of Parameters -->
56B MoE

## model_creator
<!--- Creator of the model -->
Mistral AI

## model_version
<!--- Used model version -->
mistralai/Mixtral-8x7B-Instruct-v0.1

## model_published
<!--- When was the model published -->
December 2023

## model_type
<!--- {api, api_with_logit, local} -->
local

## model_link
<!--- Link to the model -->
https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1

## model_description
<!--- Short description of the model -->
The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. It achieved strong performances on a wide range of benchmarks, often outperforming models dense models such as Llama 2 70B with notably larger parameter counts.

# environment

## num_gpus
<!--- number of gpu's used --> 
-

## gpu_power_draw
<!--- draw of the used GPUs in kW --> 
-

## time_to_train
<!--- total time taken for training in hours --> 
-

## datacenter_carbon_intensity
<!--- grams of CO2 emissions per kWh of energy consumed of the datacenter -->
-