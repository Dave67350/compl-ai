data_config: &data_config
  type: "toxicity_advbench"
  debug: false
  subset_size: 520

config:
  run_id: 1
  model: !include models/default_model.yaml
  seed: 36
  benchmark_configs:
    - name: "toxicity_advbench"
      type: "toxicity_advbench"
      data_config: *data_config
      provider: "hf"
      num_workers: 1
      postprocessor: 
        type: "toxicity_advbench"

info:
  license: "MIT License"
  source: "https://github.com/llm-attacks/llm-attacks"
